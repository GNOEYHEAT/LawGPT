{"cells":[{"cell_type":"markdown","metadata":{"tags":[],"id":"jA-PlQO3lEd3"},"source":["# LawGPT"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0CzQjJ7zla0R","executionInfo":{"status":"ok","timestamp":1685622651411,"user_tz":-540,"elapsed":3606,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}},"outputId":"f75529c2-da4c-4be1-91e8-0c019835df8f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install transformers\n","!pip install accelerate # kakaobrain/kogpt 사용에 필요함.\n","# !pip install pytorch_lightning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1MaXkvJIloS4","executionInfo":{"status":"ok","timestamp":1685622666043,"user_tz":-540,"elapsed":14634,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}},"outputId":"ebaefd7d-a1a9-40f3-db6a-e9a78ea0569c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.19.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"6wxI-5I4lEd5","executionInfo":{"status":"ok","timestamp":1685622671125,"user_tz":-540,"elapsed":5084,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c0d08407-cfc1-478b-cbec-df3c4e26c42b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":3}],"source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# %matplotlib inline\n","\n","plt.rc('font', family='Malgun Gothic')\n","plt.rc('axes', unicode_minus=False)\n","\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","# import pytorch_lightning as pl\n","\n","import transformers\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","transformers.logging.set_verbosity_error()\n","\n","import re\n","from tqdm import tqdm\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from argparse import ArgumentParser\n","\n","# import wandb\n","# from pytorch_lightning.loggers import WandbLogger\n","# wandb_logger = WandbLogger(name=\"LawGPT\", project=\"LawGPT\")\n","\n","parser = ArgumentParser(description=\"LawGPT\")\n","parser.add_argument('--text_pretrained_model', default=\"kogpt2\", type=str)\n","parser.add_argument('--text_len', default=100, type=int)\n","parser.add_argument('--learning_rate', default=0.00005, type=float)\n","parser.add_argument('--batch_size', default=32, type=int)\n","parser.add_argument('--epochs', default=10, type=int)\n","parser.add_argument('--seed', default=826, type=int)\n","args = parser.parse_args('')\n","\n","# wandb.config.update(args)\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","text_len = args.text_len\n","BATCH_SIZE = args.batch_size\n","EPOCHS = args.epochs\n","SEED = args.seed\n","\n","def set_seeds(seed=SEED):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # pl.seed_everything(SEED)\n","\n","set_seeds()\n","\n","device"]},{"cell_type":"markdown","metadata":{"id":"I1LJtACWlEd6"},"source":["## run.py"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"sveAprEplEd6","executionInfo":{"status":"ok","timestamp":1685622676034,"user_tz":-540,"elapsed":4911,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}}},"outputs":[],"source":["Q_TKN = \"<usr>\"\n","A_TKN = \"<sys>\"\n","BOS = '</s>'\n","EOS = '</s>'\n","MASK = '<unused0>'\n","SENT = '<unused1>'\n","PAD = '<pad>'\n","\n","if args.text_pretrained_model == \"polyglot-ko\":\n","    text_pretrained_model = \"EleutherAI/polyglot-ko-1.3b\"\n","    tokenizer = AutoTokenizer.from_pretrained(text_pretrained_model,\n","                                              bos_token=BOS, eos_token=EOS, unk_token=\"<unk>\", pad_token=PAD, mask_token=MASK)\n","    model = AutoModelForCausalLM.from_pretrained(text_pretrained_model)\n","\n","if args.text_pretrained_model == \"kogpt6b-ryan1.5b\":\n","    text_pretrained_model = \"kakaobrain/kogpt\"\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n","        bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]'\n","    )\n","    model = AutoModelForCausalLM.from_pretrained(\n","        'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n","        pad_token_id=tokenizer.eos_token_id,\n","        torch_dtype='auto', low_cpu_mem_usage=True\n","    )\n","\n","if args.text_pretrained_model == \"kogpt2\":\n","    text_pretrained_model = \"skt/kogpt2-base-v2\"\n","    tokenizer = AutoTokenizer.from_pretrained(text_pretrained_model,\n","                                              bos_token=BOS, eos_token=EOS, unk_token=\"<unk>\", pad_token=PAD, mask_token=MASK)\n","    model = AutoModelForCausalLM.from_pretrained(text_pretrained_model)   "]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"ceYLRexalEd7","executionInfo":{"status":"ok","timestamp":1685622676034,"user_tz":-540,"elapsed":7,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}},"outputId":"ba2084d4-214c-4eb7-eb7e-5fa5b926ba5f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                판시사항  \\\n","0  [1] 매도인이 악의인 계약명의신탁의 명의수탁자로부터 명의신탁의 목적물인 주택을 임...   \n","1  임차인이 주택임대차보호법 제6조의3 제1항 본문에 따라 계약갱신을 요구하였더라도 임...   \n","2  [1] 대지에 관한 저당권 설정 후 건물이 신축되고 그 신축건물에 다시 저당권이 설...   \n","3  전세권과 임차권의 차이점 추신영 동방문화사전세권과 주택임대차의 대항력과 우선변제권 ...   \n","4  가. 구 주택임대차보호법 (1981.3.5. 법률 제3379호) 부칙 제2항 단서에...   \n","\n","                                                판결요지  \n","0  [1] 매도인이 악의인 계약명의신탁에서 명의수탁자로부터 명의신탁의 목적물인 주택을 ...  \n","1  주택임대차보호법 제6조, 제6조의3 등 관련 규정의 내용과 체계, 입법 취지 등을 ...  \n","2  [1] 대지에 관한 저당권 설정 후에 비로소 건물이 신축되고 그 신축건물에 대하여 ...  \n","3  - 전세권과 임차권의 차이점 (판례강의) 계약법 / 동방문화사 2015 오경미 채권...  \n","4  가. 구 주택임대차보호법 (1981.3.5. 법률 제3379호) 부칙 제2항 단서는...  "],"text/html":["\n","  <div id=\"df-1a110f4b-8aab-49cb-ac6a-fb6db251507e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>판시사항</th>\n","      <th>판결요지</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[1] 매도인이 악의인 계약명의신탁의 명의수탁자로부터 명의신탁의 목적물인 주택을 임...</td>\n","      <td>[1] 매도인이 악의인 계약명의신탁에서 명의수탁자로부터 명의신탁의 목적물인 주택을 ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>임차인이 주택임대차보호법 제6조의3 제1항 본문에 따라 계약갱신을 요구하였더라도 임...</td>\n","      <td>주택임대차보호법 제6조, 제6조의3 등 관련 규정의 내용과 체계, 입법 취지 등을 ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[1] 대지에 관한 저당권 설정 후 건물이 신축되고 그 신축건물에 다시 저당권이 설...</td>\n","      <td>[1] 대지에 관한 저당권 설정 후에 비로소 건물이 신축되고 그 신축건물에 대하여 ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>전세권과 임차권의 차이점 추신영 동방문화사전세권과 주택임대차의 대항력과 우선변제권 ...</td>\n","      <td>- 전세권과 임차권의 차이점 (판례강의) 계약법 / 동방문화사 2015 오경미 채권...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>가. 구 주택임대차보호법 (1981.3.5. 법률 제3379호) 부칙 제2항 단서에...</td>\n","      <td>가. 구 주택임대차보호법 (1981.3.5. 법률 제3379호) 부칙 제2항 단서는...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a110f4b-8aab-49cb-ac6a-fb6db251507e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1a110f4b-8aab-49cb-ac6a-fb6db251507e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1a110f4b-8aab-49cb-ac6a-fb6db251507e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}],"source":["df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/precedent_preprocess.csv\")\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"sAlP6LUjlEd7"},"source":["## preprocessing.py"]},{"cell_type":"code","source":["train_num = round(len(df)*0.7)\n","val_num = round(len(df)*0.2)\n","test_num = round(len(df)*0.1)\n","\n","train_num, val_num, test_num"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CoePZTiIWOXr","executionInfo":{"status":"ok","timestamp":1685622676034,"user_tz":-540,"elapsed":6,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}},"outputId":"84316ccf-a0cb-4aeb-da73-e73248742d65"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(270, 77, 39)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["X_train = df[\"판시사항\"].iloc[:train_num]\n","y_train = df[\"판결요지\"].iloc[:train_num]\n","\n","X_val= df[\"판시사항\"].iloc[train_num:train_num+val_num]\n","y_val = df[\"판결요지\"].iloc[train_num:train_num+val_num]\n","\n","X_test = df[\"판시사항\"].iloc[train_num+val_num:]\n","y_test = df[\"판결요지\"].iloc[train_num+val_num:]\n","\n","X_train.shape, X_val.shape, X_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2FaKhG8CuBSr","executionInfo":{"status":"ok","timestamp":1685622676035,"user_tz":-540,"elapsed":6,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}},"outputId":"2a590749-3b2e-45ee-c62f-8331564ed089"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((270,), (77,), (39,))"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["### Example"],"metadata":{"id":"rotk_GKao2tt"}},{"cell_type":"code","source":["model.to(device='cuda', non_blocking=True)\n","_ = model.eval()\n","\n","sample_idx = 1  # 예제 번호\n","\n","prompt = X_train[sample_idx][:text_len]\n","with torch.no_grad():\n","  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n","  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=text_len)\n","  generated = tokenizer.batch_decode(gen_tokens)[0]\n","\n","print(\"*** True Data ***\")\n","print(X_train[sample_idx])\n","\n","print(\"*** Input Data ***\")\n","print(X_train[sample_idx][:text_len])\n","\n","print(\"*** Generated Data ***\")\n","print(generated)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iA2YvE2eoBYO","executionInfo":{"status":"ok","timestamp":1685622684978,"user_tz":-540,"elapsed":8948,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}},"outputId":"6706e91b-ea6f-4a38-a5f2-13b1c713d234"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["*** True Data ***\n","임차인이 주택임대차보호법 제6조의3 제1항 본문에 따라 계약갱신을 요구하였더라도 임대인이나 같은 법 제3조 제4항에 따라 임대인의 지위를 승계한 임차주택의 양수인이 같은 법 제6조 제1항 전단에서 정한 기간 내에 제6조의3 제1항 단서 제8호에 따라 주택에 실제 거주하려고 한다는 사유를 들어 임차인의 계약갱신 요구를 거절할 수 있는지 여부(원칙적 적극)\n","*** Input Data ***\n","임차인이 주택임대차보호법 제6조의3 제1항 본문에 따라 계약갱신을 요구하였더라도 임대인이나 같은 법 제3조 제4항에 따라 임대인의 지위를 승계한 임차주택의 양수인이 같은 법 제6조\n","*** Generated Data ***\n","임차인이 주택임대차보호법 제6조의3 제1항 본문에 따라 계약갱신을 요구하였더라도 임대인이나 같은 법 제3조 제4항에 따라 임대인의 지위를 승계한 임차주택의 양수인이 같은 법 제6조 제1항 본문의 본문에 규정된 임차주택의 양수인을 임대인으로 보지 아니한다.\n","또한 임대인이 다른 임대주택의 양수인에게 임차주택의 양수를 청구할 때에도 임대인은 임대인으로서 임차주택의 양수인이 임대인의 지위를 승계한 임차주택의 양수인에게 임차주택의 양수\n"]}]},{"cell_type":"markdown","metadata":{"id":"2W7YN5G2lEd7"},"source":["## data_loader.py"]},{"cell_type":"code","source":["# 챗봇 데이터를 처리하는 클래스를 만든다.\n","class ChatbotDataset(Dataset):\n","    def __init__(self, x_chats, y_chats, max_len=args.text_len):  # 데이터셋의 전처리를 해주는 부분\n","        self.x_data = x_chats\n","        self.y_data = y_chats\n","        self.max_len = max_len\n","        self.q_token = Q_TKN\n","        self.a_token = A_TKN\n","        self.sent_token = SENT\n","        self.eos = EOS\n","        self.mask = MASK\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):  # chatbotdata 의 길이를 리턴한다.\n","        return len(self.x_data)\n","\n","    def __getitem__(self, idx): # 로드한 챗봇 데이터를 차례차례 DataLoader로 넘겨주는 메서드\n","        q = self.x_data[idx]  # 질문을 가져온다.\n","        q = re.sub(r\"([?.!,])\", r\" \", q)  # 구둣점들을 제거한다.\n","\n","        a = self.y_data[idx]  # 답변을 가져온다.\n","        a = re.sub(r\"([?.!,])\", r\" \", a)  # 구둣점들을 제거한다.\n","\n","        q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token)\n","        q_len = len(q_toked)\n","\n","        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)\n","        a_len = len(a_toked)\n","\n","        #질문의 길이가 최대길이보다 크면\n","        if q_len > self.max_len:\n","            a_len = self.max_len - q_len  # 답변의 길이를 최대길이 - 질문길이\n","            if a_len <= 0:  #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n","                q_toked = q_toked[-(int(self.max_len / 2)) :]   #질문길이를 최대길이의 반으로 \n","                q_len = len(q_toked)\n","                a_len = self.max_len - q_len              #답변의 길이를 최대길이 - 질문길이\n","            a_toked = a_toked[:a_len]\n","            a_len = len(a_toked)\n","\n","        # 질문의 길이 + 답변의 길이가 최대길이보다 크면\n","        if q_len + a_len > self.max_len:\n","            a_len = self.max_len - q_len  # 답변의 길이를 최대길이 - 질문길이\n","            if a_len <= 0:  # 질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n","                q_toked = q_toked[-(int(self.max_len / 2)) :] # 질문길이를 최대길이의 반으로 \n","                q_len = len(q_toked)\n","                a_len = self.max_len - q_len  # 답변의 길이를 최대길이 - 질문길이\n","            a_toked = a_toked[:a_len]\n","            a_len = len(a_toked)\n","\n","        # 답변 labels = [mask, mask, ...., mask, ..., <bos>,..답변.. <eos>, <pad>....]\n","        labels = [self.mask,] * q_len + a_toked[1:]\n","\n","        # mask = 질문길이 0 + 답변길이 1 + 나머지 0\n","        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)\n","        # 답변 labels을 index 로 만든다.\n","        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n","        # 최대길이만큼 PADDING\n","        while len(labels_ids) < self.max_len:\n","            labels_ids += [self.tokenizer.pad_token_id]\n","\n","        # 질문 + 답변을 index 로 만든다.    \n","        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n","        # 최대길이만큼 PADDING\n","        while len(token_ids) < self.max_len:\n","            token_ids += [self.tokenizer.pad_token_id]\n","\n","        # 질문+답변, 마스크, 답변\n","        return (token_ids, np.array(mask), labels_ids)"],"metadata":{"id":"xf7zSoP3u0Vy","executionInfo":{"status":"ok","timestamp":1685622684978,"user_tz":-540,"elapsed":11,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def collate_batch(batch):\n","    data = [item[0] for item in batch]\n","    mask = [item[1] for item in batch]\n","    label = [item[2] for item in batch]\n","    return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)"],"metadata":{"id":"g6F_tPdEYS9D","executionInfo":{"status":"ok","timestamp":1685622684979,"user_tz":-540,"elapsed":11,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"HnMUbhkSlEd8","executionInfo":{"status":"ok","timestamp":1685622684979,"user_tz":-540,"elapsed":11,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}}},"outputs":[],"source":["train_set = ChatbotDataset(X_train, y_train, max_len=args.text_len)\n","train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, num_workers=0, shuffle=True, collate_fn=collate_batch,)"]},{"cell_type":"code","source":["print(\"start\")\n","for batch_idx, samples in enumerate(train_dataloader):\n","    token_ids, mask, label = samples\n","    print(\"token_ids ====> \", token_ids)\n","    print(\"mask =====> \", mask)\n","    print(\"label =====> \", label)\n","    break\n","print(\"end\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PBWowPxMprAG","executionInfo":{"status":"ok","timestamp":1685622684979,"user_tz":-540,"elapsed":11,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}},"outputId":"889c469a-8c29-4a7c-eff0-78edd49f9991"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["start\n","token_ids ====>  tensor([[    2,  9028,   739,  ..., 10032,  9436, 28623],\n","        [    2, 24433,  7627,  ...,   739, 15064,  7832],\n","        [19458, 13068,  9022,  ..., 24521, 50218, 19271],\n","        ...,\n","        [    2,  9175, 16365,  ...,    10,     4,  9175],\n","        [28334, 27606,  9249,  ...,  7416,  7743,  9554],\n","        [ 8340,  8213,  8213,  ...,  8758,  8346,  9833]])\n","mask =====>  tensor([[0, 0, 0,  ..., 1, 1, 1],\n","        [0, 0, 0,  ..., 1, 1, 1],\n","        [0, 0, 0,  ..., 1, 1, 1],\n","        ...,\n","        [0, 0, 0,  ..., 0, 1, 1],\n","        [0, 0, 0,  ..., 1, 1, 1],\n","        [0, 0, 0,  ..., 1, 1, 1]])\n","label =====>  tensor([[    9,     9,     9,  ...,  9436, 28623,     3],\n","        [    9,     9,     9,  ..., 15064,  7832,     3],\n","        [    9,     9,     9,  ..., 50218, 19271,     3],\n","        ...,\n","        [    9,     9,     9,  ...,     9,  9175,     3],\n","        [    9,     9,     9,  ...,  7743,  9554,     3],\n","        [    9,     9,     9,  ...,  8346,  9833,     3]])\n","end\n"]}]},{"cell_type":"markdown","metadata":{"id":"DbyaFvTLlEd8"},"source":["## model.py"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"JLXmAe04lEd8","executionInfo":{"status":"ok","timestamp":1685622684980,"user_tz":-540,"elapsed":11,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}}},"outputs":[],"source":["model.to(device)\n","model.train()\n","\n","learning_rate = args.learning_rate\n","criterion = nn.CrossEntropyLoss(reduction=\"none\")\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","Sneg = -1e18"]},{"cell_type":"markdown","metadata":{"id":"T-Y4slsdlEd8"},"source":["## main.py"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"QiQumPFplEd8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685622765975,"user_tz":-540,"elapsed":81006,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}},"outputId":"7881c0ab-ad83-4bff-a9b0-f2302c7e7af6"},"outputs":[{"output_type":"stream","name":"stdout","text":["start\n","\n","epoch:  0\n"]},{"output_type":"stream","name":"stderr","text":["9it [00:07,  1.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["loss:  tensor(14.9341, device='cuda:0', grad_fn=<DivBackward0>)\n","epoch:  1\n"]},{"output_type":"stream","name":"stderr","text":["9it [00:08,  1.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["loss:  tensor(12.9049, device='cuda:0', grad_fn=<DivBackward0>)\n","epoch:  2\n"]},{"output_type":"stream","name":"stderr","text":["9it [00:07,  1.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["loss:  tensor(16.7840, device='cuda:0', grad_fn=<DivBackward0>)\n","epoch:  3\n"]},{"output_type":"stream","name":"stderr","text":["9it [00:07,  1.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["loss:  tensor(16.6980, device='cuda:0', grad_fn=<DivBackward0>)\n","epoch:  4\n"]},{"output_type":"stream","name":"stderr","text":["9it [00:07,  1.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["loss:  tensor(16.2718, device='cuda:0', grad_fn=<DivBackward0>)\n","epoch:  5\n"]},{"output_type":"stream","name":"stderr","text":["9it [00:08,  1.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["loss:  tensor(19.0806, device='cuda:0', grad_fn=<DivBackward0>)\n","epoch:  6\n"]},{"output_type":"stream","name":"stderr","text":["9it [00:07,  1.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["loss:  tensor(16.2848, device='cuda:0', grad_fn=<DivBackward0>)\n","epoch:  7\n"]},{"output_type":"stream","name":"stderr","text":["9it [00:07,  1.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["loss:  tensor(18.8570, device='cuda:0', grad_fn=<DivBackward0>)\n","epoch:  8\n"]},{"output_type":"stream","name":"stderr","text":["9it [00:07,  1.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["loss:  tensor(18.8013, device='cuda:0', grad_fn=<DivBackward0>)\n","epoch:  9\n"]},{"output_type":"stream","name":"stderr","text":["9it [00:07,  1.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["loss:  tensor(12.1994, device='cuda:0', grad_fn=<DivBackward0>)\n","\n","end\n"]}],"source":["print(\"start\")\n","print(\"\")\n","\n","for epoch in range(EPOCHS):\n","    print(\"epoch: \", epoch)\n","    for batch_idx, samples in tqdm(enumerate(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids, mask, label = [t.to(device) for t in samples]\n","        out = model(token_ids)\n","        out = out.logits # Returns a new tensor with the logit of the elements of input\n","        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)\n","        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out))\n","        loss = criterion(mask_out.transpose(2, 1), label)\n","        # 평균 loss 만들기 avg_loss[0] / avg_loss[1] <- loss 정규화\n","        avg_loss = loss.sum() / mask.sum()\n","        \n","        avg_loss.backward()\n","        # 학습 끝\n","        optimizer.step()\n","    print(\"loss: \", avg_loss)\n","\n","print(\"\")\n","print(\"end\")"]},{"cell_type":"code","source":["# import datetime as dt\n","\n","# now = dt.datetime.now()\n","# nowDate = now.strftime('%m%d_%H%M')\n","\n","# torch.save(model.state_dict(), 'models/' + str(nowDate) + '_dict.bin')\n","# torch.save(model, 'models/' + str(nowDate) + '_model.bin')"],"metadata":{"id":"0DOU_LzEnBXw","executionInfo":{"status":"ok","timestamp":1685622765975,"user_tz":-540,"elapsed":14,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["X_train[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"cj88Cvr7xDH3","executionInfo":{"status":"ok","timestamp":1685622765975,"user_tz":-540,"elapsed":14,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}},"outputId":"e6ab0268-4276-45c7-efe1-ed9e40c398e3"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'임차인이 주택임대차보호법 제6조의3 제1항 본문에 따라 계약갱신을 요구하였더라도 임대인이나 같은 법 제3조 제4항에 따라 임대인의 지위를 승계한 임차주택의 양수인이 같은 법 제6조 제1항 전단에서 정한 기간 내에 제6조의3 제1항 단서 제8호에 따라 주택에 실제 거주하려고 한다는 사유를 들어 임차인의 계약갱신 요구를 거절할 수 있는지 여부(원칙적 적극)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["y_train[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"ZolwvaWK1c_I","executionInfo":{"status":"ok","timestamp":1685622765975,"user_tz":-540,"elapsed":13,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}},"outputId":"b7f9542f-7fa8-42f6-9cd2-6b0a1a159ee8"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'주택임대차보호법 제6조, 제6조의3 등 관련 규정의 내용과 체계, 입법 취지 등을 종합하여 보면, 임차인이 같은 법 제6조의3 제1항 본문에 따라 계약갱신을 요구하였더라도, 임대인으로서는 특별한 사정이 없는 한 같은 법 제6조 제1항 전단에서 정한 기간 내라면 제6조의3 제1항 단서 제8호에 따라 임대인이 목적 주택에 실제 거주하려고 한다는 사유를 들어 임차인의 계약갱신 요구를 거절할 수 있고, 같은 법 제3조 제4항에 의하여 임대인의 지위를 승계한 임차주택의 양수인도 그 주택에 실제 거주하려는 경우 위 갱신거절 기간 내에 위 제8호에 따른 갱신거절 사유를 주장할 수 있다고 보아야 한다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["with torch.no_grad():\n","    while 1:\n","        q = input(\"user > \").strip()\n","        if q == \"quit\":\n","            break\n","        a = \"\"\n","        while 1:\n","            input_ids = torch.LongTensor(tokenizer.encode(Q_TKN + q + SENT + A_TKN + a)).unsqueeze(dim=0).to(device)\n","            pred = model(input_ids)\n","            pred = pred.logits\n","            gen = tokenizer.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().cpu().numpy().tolist())[-1]\n","            if gen == EOS:\n","                break\n","            a += gen.replace(\"▁\", \" \")\n","        print(\"Chatbot > {}\".format(a.strip()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"9c_M_pv-d88p","executionInfo":{"status":"error","timestamp":1685623092052,"user_tz":-540,"elapsed":326080,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}},"outputId":"ccd53251-9e8c-4098-8633-bfa37bb99cab"},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":["user > 임차인이 주택임대차보호법 제6조의3 제1항 본문에 따라 계약갱신을 요구하였더라도 임대인이나 같은 법 제3조 제4항에 따라 임대인의 지위를 승계한 임차주택의 양수인이 같은 법 제6조 제1항 전단에서 정한 기간 내에 제6조의3 제1항 단서 제8호에 따라 주택에 실제 거주하려고 한다는 사유를 들어 임차인의 계약갱신 요구를 거절할 수 있는지 여부(원칙적 적극)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m:\u001b[94m9\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_gpt2.py\u001b[0m:\u001b[94m1076\u001b[0m in        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1073 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1074 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1075 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1076 \u001b[2m│   │   \u001b[0mtransformer_outputs = \u001b[96mself\u001b[0m.transformer(                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1077 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids,                                                                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1078 \u001b[0m\u001b[2m│   │   │   \u001b[0mpast_key_values=past_key_values,                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1079 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_gpt2.py\u001b[0m:\u001b[94m900\u001b[0m in \u001b[92mforward\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 897 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mencoder_attention_mask,                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 898 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 899 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 900 \u001b[2m│   │   │   │   \u001b[0moutputs = block(                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 901 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhidden_states,                                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 902 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mlayer_past=layer_past,                                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 903 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mattention_mask=attention_mask,                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_gpt2.py\u001b[0m:\u001b[94m390\u001b[0m in \u001b[92mforward\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 387 \u001b[0m\u001b[2m│   \u001b[0m) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 388 \u001b[0m\u001b[2m│   │   \u001b[0mresidual = hidden_states                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 389 \u001b[0m\u001b[2m│   │   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.ln_1(hidden_states)                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 390 \u001b[2m│   │   \u001b[0mattn_outputs = \u001b[96mself\u001b[0m.attn(                                                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 391 \u001b[0m\u001b[2m│   │   │   \u001b[0mhidden_states,                                                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 392 \u001b[0m\u001b[2m│   │   │   \u001b[0mlayer_past=layer_past,                                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 393 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_gpt2.py\u001b[0m:\u001b[94m331\u001b[0m in \u001b[92mforward\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 328 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.reorder_and_upcast_attn:                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 329 \u001b[0m\u001b[2m│   │   │   \u001b[0mattn_output, attn_weights = \u001b[96mself\u001b[0m._upcast_and_reordered_attn(query, key, valu  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 330 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 331 \u001b[2m│   │   │   \u001b[0mattn_output, attn_weights = \u001b[96mself\u001b[0m._attn(query, key, value, attention_mask, he  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 332 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 333 \u001b[0m\u001b[2m│   │   \u001b[0mattn_output = \u001b[96mself\u001b[0m._merge_heads(attn_output, \u001b[96mself\u001b[0m.num_heads, \u001b[96mself\u001b[0m.head_dim)       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 334 \u001b[0m\u001b[2m│   │   \u001b[0mattn_output = \u001b[96mself\u001b[0m.c_proj(attn_output)                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_gpt2.py\u001b[0m:\u001b[94m201\u001b[0m in \u001b[92m_attn\u001b[0m   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 198 \u001b[0m\u001b[2m│   │   │   \u001b[0mmask_value = torch.finfo(attn_weights.dtype).min                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 199 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scala\u001b[0m  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 200 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be\u001b[0m  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 201 \u001b[2m│   │   │   \u001b[0mmask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(attn_we  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 202 \u001b[0m\u001b[2m│   │   │   \u001b[0mattn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype),  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 203 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 204 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m attention_mask \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                    \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mRuntimeError: \u001b[0mCUDA error: device-side assert triggered\n","CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be \n","incorrect.\n","For debugging consider passing \u001b[33mCUDA_LAUNCH_BLOCKING\u001b[0m=\u001b[1;36m1\u001b[0m.\n","Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">9</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gpt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1076</span> in        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1073 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1074 │   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1075 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1076 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>transformer_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transformer(                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1077 │   │   │   </span>input_ids,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1078 │   │   │   </span>past_key_values=past_key_values,                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1079 │   │   │   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gpt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">900</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 897 │   │   │   │   │   </span>encoder_attention_mask,                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 898 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 899 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 900 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>outputs = block(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 901 │   │   │   │   │   </span>hidden_states,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 902 │   │   │   │   │   </span>layer_past=layer_past,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 903 │   │   │   │   │   </span>attention_mask=attention_mask,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gpt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">390</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 387 │   </span>) -&gt; Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 388 │   │   </span>residual = hidden_states                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 389 │   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ln_1(hidden_states)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 390 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attn_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.attn(                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 391 │   │   │   </span>hidden_states,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 392 │   │   │   </span>layer_past=layer_past,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 393 │   │   │   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gpt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">331</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 328 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.reorder_and_upcast_attn:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 329 │   │   │   </span>attn_output, attn_weights = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._upcast_and_reordered_attn(query, key, valu  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 330 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 331 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>attn_output, attn_weights = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._attn(query, key, value, attention_mask, he  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 332 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 333 │   │   </span>attn_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._merge_heads(attn_output, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_heads, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.head_dim)       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 334 │   │   </span>attn_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.c_proj(attn_output)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gpt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">201</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_attn</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 198 │   │   │   </span>mask_value = torch.finfo(attn_weights.dtype).min                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 199 │   │   │   # Need to be a tensor, otherwise we get error: `RuntimeError: expected scala</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 200 │   │   │   # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 201 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>mask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(attn_we  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 202 │   │   │   </span>attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype),  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 203 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 204 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> attention_mask <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>CUDA error: device-side assert triggered\n","CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be \n","incorrect.\n","For debugging consider passing <span style=\"color: #808000; text-decoration-color: #808000\">CUDA_LAUNCH_BLOCKING</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.\n","Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["print(\"result\")\n","print(a.strip())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kUD254e22xA2","executionInfo":{"status":"ok","timestamp":1685623098577,"user_tz":-540,"elapsed":384,"user":{"displayName":"Taehyeong Kim","userId":"14456942408080601243"}},"outputId":"279ae191-989a-43f3-c3ab-8e7b8daf9a32"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["result\n","주택임대차보호법 제6조의3 제1항  구 주택임대차보호법 제6조의3 등<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>에서 임차인이<pad><pad><pad><pad><pad><pad><pad><pad>에서 임차<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> 더 후<pad> 전출입신고를 한 후<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> 더 후<pad><pad><pad><pad><pad><pad><pad><pad><pad> 더 후<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> 더<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>에서 또 다른 자녀와 함께’를 자녀와 함께’라고 규정하고 있는<pad><pad><pad><pad> 자녀와 함께<pad><pad><pad><pad> 부모가 아닌 자녀와 함께 또는 다른 자녀와 함께’를’를<pad><pad><pad><pad><pad><pad><pad> 자녀와 함께<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> 자녀와 함께 또는 또는 다른 자녀와 함께’를 자녀와 함께<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> 가족과 함께 또는 가족과 함께’를<pad><pad> 가족과 함께’를 따로’를 따로’를 따로’를 따로 따로’를 따로’를 따로’라고 따로’라고 따로 따로 따로 따로 따로 따로 따로 따로 따로’라고 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로 따로\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":0}